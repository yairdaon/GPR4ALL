
\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{algorithm} % algorithm package


\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text





%----------------------------------------------------------------------------------------
% new commands
%----------------------------------------------------------------------------------------
\DeclareMathOperator*{\argmax}{arg\,max}

% boldface caligraphic etc
\newcommand{\ee}{\mathbb{E}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\logl}{\mathbf{l}}

\newcommand{\xx}{\mathbf{x}}
\newcommand{\cov}{\text{cov}}
\newcommand{\dd}{\mathcal{D}}


% GP notation
\newcommand{\egp}{\mathbb{E}_{\mathcal{GP}}}
\newcommand{\en}{\mathbb{E}_{f(\xn)}}
\newcommand{\exo}{\mathbb{E}_{(\xn,\omega)}}
\newcommand{\var}{\text{Var}}
\newcommand{\vgp}{\text{Var}_{\mathcal{GP}}}

\newcommand{\bars}{ \ \ \ \ \ \ \ \ \ \ } 
\newcommand{\ddxni}{\frac{\partial}{   \partial x_i^{(n)} }} % partial wrt ith coordinate of x^(n)
\newcommand{\gxn}{\nabla_{x^{(n)}}} % gradient wrt x^(n)


\newcommand{\xn}{x^{(n)}} % nth x
\newcommand{\xj}{x^{(j)}}
\newcommand{\xii}{x^{(i)}}
\newcommand{\Xn}{X^{(n)}}


\newcommand{\mn}{m^{(n)}} 
\newcommand{\mnm}{m^{(n-1)}}
\newcommand{\mnxn}{m^{(n-1)}(\xn)}
\newcommand{\mii}{m^{(i)}}
\newcommand{\mj}{m^{(j)}}
\newcommand{\Mn}{M^{(n)}}


\newcommand{\fn}{f^{(n)}} % nth f
\newcommand{\fnm}{f^{(n-1)}}
\newcommand{\fmm}{\bar{f}}
\newcommand{\fii}{f^{(i)}}
\newcommand{\fj}{f^{(j)}}
\newcommand{\Fn}{F^{(n)}}
\newcommand{\Fnm}{F^{(n-1)}}


\newcommand{\kn}{k^{(n)}}
\newcommand{\knm}{k^{(n-1)}}


\newcommand{\Kinvn}{K^{(-n)}} % K^(-n)
\newcommand{\Kinvnm}{K^{(-[n-1])}} 
\newcommand{\Kinv}{K^{-1}} % K inverse


\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}

\newcommand{\sqn}{\sigma ^{2 , (n)}     }
\newcommand{\sqnm}{\sigma ^{2 , (n-1)}     }
\newcommand{\signxn}{\sigma^{2, (n-1)} (\xn)}

\newcommand{\vns}{v(s ; \xn)}
\newcommand{\vnssqr}{v^2(s;\xn)}

\newcommand{\norm}[1]{||#1||} %norm
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------


\begin{document}
\section{Intro}
I derive the objective function for the optimal experimental design. The first five sections are preparations that will be used in the
long calculation that follows.

\section{Notation}
Some notation I will use:
\begin{itemize}
 \item $\rr^d$ is where our probability lives - $p :\rr^d \to \rr_{+}$.
 \item $(\Omega, \mathcal{F}, \mathbb{P})$ the probability space of the gaussian process. Changes as we condition.
 \item $f : \rr^d \times \Omega \to \rr$ a gaussian process. 
 \item Superscript in parenthesis is denotes a number of ``observations''. 
 \item Subscript is coordinate: $\xj_i$ is the $i$th coordinate of $\xj \in \rr^d$.
 \item $\xj \in \rr^d$ is the $j$th vector (point) in which we calcualte $\phi$.
 \item $\fj : = \phi ( \xj ) - m(\xj)$: the true value at $\xj$ minus the prior.
 \item $m: \rr ^d \to \rr$ the mean function. 
 \item $\mn: \rr^d \to \rr$ is the posterior mean (interpolant) after observing $\fj = \phi (\xj) , j=1,...,n$.
 \item $k: \rr ^d \times \rr ^d \to \rr$ the covariance function (kernel).
 \item $\kn: \rr^d \to \rr^n$ is such that $\kn_j(s) = k(\xj , s)$ where $1 \leq j \leq n$. 
 \item $K^{(n)} \in \rr^{ n\times n} $ is a Gram matrix: $K^{(n)}_{ij} = k(\xii , \xj)$.
 \item $\Kinvn = [K^{(n)}]^{-1}$.
 \item $\Kinvnm = [K^{(n-1)}]^{-1}$.
 \item Let $V: \rr^d \to \rr$ and $x \in \rr^d$. Then $\egp[ V(x) ]=\egp [V(x, \cdot)] =  \int_{\Omega} V(x, \xi) \mathbb{P}(d\xi) $
 \item A collection of observations is capital. So $\Xn = ( x^{(1)} , ... , \xn )$, $\Fn = (f^{(1)} ,..., \fn)$  and $ \Mn = (m^{(1)} , ..., \mn)$.
\end{itemize}




\section{Block Inversion}
We need to invert covariance matrices. Moreover, we need to invert
$$ \left( \begin{array}{cc}
K   & k \\
k^t & c \end{array} \right)
$$

where $K$ is a symmetric matrix, $k$ is a vector and $c$ is a scalar.
The inverse is given by:

$$ \left( \begin{array}{cc}
\bar{K}   &  \bar{k} \\
\bar{k}^t   &  \bar{c} \end{array} \right)
$$
with:
\begin{align}
 \begin{split}
\bar{K} &=  \Kinv + K^{-1}k\bar{c}k^t\Kinv\\
\bar{k} &=  -\Kinv k\bar{c}\\
\bar{c} &= (c - k^t\Kinv k)^{-1} 
 \end{split}
\end{align}

\section{A Lemma}
We use the block inversion to prove the following:
\begin{lemma}
Assume $\fn = \mnm(\xn) =  \knm(\xn)K^{(-[n-1])}\Fnm$. Then $\mnm(s) = \mn(s)$.
\end{lemma}
\begin{proof}
 For this proof we fix $s\in \rr ^{d}$ and denote:
\begin{itemize}
 \item $\Kinv := \Kinvnm$ 
 \item $k : = \knm(\xn)$
 \item $c := k(\xn,\xn)$
 \item $f = \fnm$ 
 \item $a : = \mnm(\xn) = k^t\Kinv f$ (using the above notation)
 \item $k(s) = \knm(s)$
 \item $\bar{c} := (c - k^t\Kinv k)^{-1}$. 
 \item $a = m(s) : = \mnm(s)$
\end{itemize}
Then $\mnm(s) = k^t\Kinv f$ and we may express $\mn(s)$ using the block inversion.


\begin{align}
 \begin{split}
%
\mn(s) &= \left( \begin{array}{cc}
 k^t(s)  & k(\xn, s)  
\end{array} \right)
%
%
\left( \begin{array}{cc}
\Kinv + \Kinv k \bar{c} k^t \Kinv  &  -\Kinv k \bar{c} \\
-k^t\Kinv \bar{c}   &  \bar{c}
 \end{array} \right)
%
%
\left( \begin{array}{cc}
 f  \\ a 
 \end{array} \right) \\\\
%
%
%
%
% new line
%
%
%
%
&= \left( \begin{array}{cc}
 k^t(s)  & k(\xn, s)
 \end{array} \right)
%
%
\left( \begin{array}{cc}
\Kinv + \Kinv k \bar{c} k^t \Kinv  &  -\bar{c}\Kinv k  \\
-\bar{c}k^t\Kinv   & \bar{c}
 \end{array} \right)
%
%
\left( \begin{array}{cc}
 f  \\ a
  \end{array} \right) \\\\
%
%
&=  k^t(s)(\Kinv + \Kinv k \bar{c}k^t \Kinv )f - a\bar{c} k^t(s) \Kinv k  -\bar{c} k(\xn,s) k^t\Kinv f + \bar{c}a k(\xn,s) \\\\
%
%%
%
&=  k^t(s)\Kinv f + \bar{c}[k^t(s)\Kinv k\cdot  k^t \Kinv f - a k^t(s) \Kinv k \\
 &\bars - k(\xn, s) k^t\Kinv f + a k(\xn,s)] \\\\
%
%
%
&= m(s) + \bar{c}[k^t(s)\Kinv k\cdot  m (\xn) -m(\xn) k^t(s) \Kinv k \\
 &\bars - k(\xn, s) m(\xn) + m(\xn) k(\xn,s)] \\\\
%
%
%
& = m(s) = \mnm (s)
\end{split}
\end{align}
\end{proof}

The bottom line is that the observation $f( \xn ,\omega) = \mnm(\xn)$ doesn't change the posterior mean (only the posterior variance).



\section{Derivatives}
Denote $u(\xn) = k(s, \xn)  - < \knm (s) ,  \Kinvnm \knm (\xn) > $, suppressing 
the dependence on $s$. 
Let  $g(\xn,s) = \frac{u^2}{\sqnm(\xn)}$.
\begin{align}
 \begin{split}
%
%
%
&\gxn g(\xn,s) \\
%
%
%
%
&=\gxn \frac{[k(s, \xn)  - < \knm (s) ,  \Kinvnm \knm (\xn) > ]^2}{\sqnm(\xn)}\\
%
%
%
%
&= \gxn \frac{ u^2 (\xn) }{\sqnm(\xn)}\\
%
%
%
&=\frac{ 2u(\xn)\gxn u(\xn) \sqnm(\xn)}{\sqnm(\xn)^2} \\
	    &\bars- \frac{\gxn \sqnm(\xn) u^2(\xn)}{\sqnm(\xn)^2} \\\\
%
%
%
&=\frac{ 2u(\xn)}{\sqnm(\xn)} \gxn u(\xn)\\
	    &\bars- \frac{u^2(\xn)}{\sqnm(\xn)^2}\gxn \sqnm(\xn) \\
%
%
%
%
&=\frac{ 2u(\xn)}{\sqnm(\xn)} \gxn k(s , \xn) \\
	    &\bars - \frac{ 2u(\xn)}{\sqnm(\xn)} \gxn< \knm (s) ,  \Kinvnm \knm (\xn) > \\
	    &\bars - \frac{u^2(\xn)}{\sqnm(\xn)^2}\gxn \sqnm(\xn) \\
%
%
%
%
&=\frac{ 2u(\xn)}{\sqnm(\xn)} \gxn k(s , \xn) \\
	    &\bars - \frac{ 2u(\xn)}{\sqnm(\xn)} \gxn u(\xn) \\
	    &\bars - \frac{u^2(\xn)}{\sqnm(\xn)^2}\gxn \sqnm(\xn) \\
%
%
%
%
%
%
&=\frac{ 2u(\xn)}{\sqnm(\xn)} \gxn [k(s , \xn) - u(\xn)] - \frac{u^2(\xn)}{\sqnm(\xn)^2}\gxn \sqnm(\xn) \\
%
%
%
%
%
%
\end{split}
\end{align}


\section{Xtras}
Start with easy sneezy.
\begin{align}
 \begin{split}
  \gxn k(\xn,s) &= \gxn c\exp(-\frac{||\xn - s||^2}{2r^2} )\\
%
&= -k(\xn,s) \gxn\frac{||\xn - s||^2}{2r^2} \\
%
&= -\frac{k(\xn,s)}{2r^2} \gxn||\xn - s||^2 \\
%
&= -\frac{k(\xn ,s)}{r^2} (\xn-s)\\
 \end{split}
\end{align}

In the following, gradient is taken with respect to $s$.

\begin{align}
 \begin{split}
  \nabla \sqn (s) &= \nabla [ c - \kn(s) \Kinvn \kn(s)]\\\\
%
%
%
&= -2 \kn(s) \Kinvn \nabla\kn(s)\\\\
%
%
%
&= -2 \sum_{i,j=1}^{n} \kn(s)_j \Kinvn_{ji} \nabla k (s, \xii )\\\\
%
%
%
&= 2 \sum_{i,j=1}^{n} k(s,\xj) \Kinvn_{ji} \frac{k(\xii ,s)}{r^2} (s-\xii)\\\\
%
%
%
&= \frac{2}{r^2} \sum_{i,j=1}^{n} k(s,\xj) \Kinvn_{ji} k(\xii ,s) (s-\xii)\\\\
 \end{split}
\end{align}

Consequently:

\begin{align}
 \begin{split}
  \gxn \signxn &= \frac{2}{r^2} \sum_{i,j=1}^{n-1} k(\xn,\xj) \Kinvnm_{ji} k(\xii ,\xn) (\xn-\xii)\\\\
 \end{split}
\end{align}


%
%%
%%%
%%%%
%%%%%
%%%%%%
%%%%%%%
%%%%%%%%
%%%%%%%%%
%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%

\section{Average Variance}
We seek $\argmin_{\xn} \en\int  \var[ \exp( \mn(s) )] ds$. This means the we first ``imagine'' we observe $f(\xn)$ and
then calculate how much this ``observation'' changes the total variance. We then average over $f(\xn)$.
Recall that the if $X \sim \mathcal{N}(\mu, \sigma^2)$, then $\mathbb{E}[e^X] = e^{\mu + \frac{\sigma^2}{4}}$ and 
$\var[e^X] = (e^{\sigma^2} - 1) e^{2\mu + \sigma^2}$:
\begin{align}
\begin{split}
%
&\en\int  \var[ \exp( f(s) )] ds  \\
%
&=\en\int  \var[ \exp( \mn(s) + \sqn(s) )] ds  \\
%%
&=\en\int (e^{\sqn(s)} -1) e^{2\mn(s)+\sqn(s)}ds\\
%
%
&=\int \en[e^{2\mn(s)}] (e^{\sqn(s)} -1) e^{\sqn(s)}ds\\
\end{split}
\end{align}

Let us focus on $\en[e^{2\mn(s)}]$ first:

\begin{align}
 \begin{split}
\en[e^{2\mn(s)}] &= \en[\exp( 2m(s) + 2\kn(s)\Kinvn \Fn]\\
%
%
%  
&= \en[\exp( 2m(s) + 2\sum_{i,j=1}^n k(s,\xii) \Kinvn_{ij} \fj]\\
%
%
%
%  
&= e^{ 2m(s) + 2\sum_{i=1}^n\sum_{j=1}^{n-1} k(s,\xii) \Kinvn_{ij} \fj} \en[\exp(2\sum_{i=1}^n k(s,\xii) \Kinvn_{in} (f(\xn) - m(\xn)))]\\
%
%
%  
&= e^{ 2m(s) + 2\sum_{i=1}^n\sum_{j=1}^{n-1} k(s,\xii) \Kinvn_{ij} \fj} \en[\exp(2\vns \fmm (\xn)) ]\\
 \end{split}
\end{align}

where we denote:
\begin{itemize}
 \item  $\vns := \sum_{i=1}^n k(s,\xii)\Kinvn_{in}$,
 \item $\fmm (\cdot) := f(\cdot) - m(\cdot)$. 
\end{itemize}

By our assumptions, $f(\xn) \sim \mathcal{N} (\mnm(\xn) , \sqnm (\xn))$ and so $\fmm (\xn) \sim \mathcal{N}(\mnm (\xn) - m(x) , \sqnm (\xn))$.

$$
    2\vns \fmm(\xn) \sim \mathcal{N}( 2\vns[\mnm(\xn) - m(\xn)] , 4\vns^2 \sqnm (\xn)),
$$

where $\sqnm (\cdot)$ is the posterior variance after making $n-1$ ``observations''.  We conclude: 

$$
    \en[e^{2\vns \fmm(\xn)}] = e^{2\vns[\mnm(\xn) - m(\xn)] + \vns^2 \sqnm (\xn) }
$$



We continue the above series of equalities:

\begin{align}
 \begin{split}
%
%
%
&=\int \en[e^{2\mn(s)}] (e^{\sqn(s)} -1) e^{\sqn(s)}ds\\
%
%
%
&=\int e^{ 2m(s) + 2\sum_{i=1}^n\sum_{j=1}^{n-1} k(s,\xii) \Kinvn_{ij} \fj} (e^{\sqn(s)} -1) e^{\sqn(s)}\\
&\bars			 e^{2\vns[\mnm(\xn) - m(\xn)] + \vns^2 \sqnm (\xn) } ds\\
%
%
%
%
&=\int \exp( 2m(s) + 2\sum_{i=1}^n\sum_{j=1}^{n-1} k(s,\xii) \Kinvn_{ij} \fj + 2\vns[\mnm(\xn) - m(\xn)] )\\ 
&\bars			 (e^{\sqn(s)} -1) \exp( \sqn(s) + \vns^2 \sqnm (\xn) ) ds\\
%
%
%
&=\int \exp( 2m(s) + 2\sum_{i=1}^n\sum_{j=1}^{n-1} k(s,\xii) \Kinvn_{ij} \fj + 2\sum_{i=1}^n k(s,\xii)\Kinvn_{in}[\mnm(\xn) - m(\xn)] )\\ 
&\bars			 (e^{\sqn(s)} -1) \exp( \sqn(s) + \vns^2 \sqnm (\xn) ) ds\\
%
%
%
&=\int \exp( 2\mnm (s) ) (e^{\sqn(s)} -1) \exp( \sqn(s) + \vns^2 \sqnm (\xn) ) ds\\
%
%
%
\end{split}
\end{align}

We now focus on the exponents using the block inversion. We take $k = \kn(\xn), K = \Kinvnm, c = k(\xn,\xn)$ and note 
that in the matrix block inversion, $\bar{c} =\frac{1}{\sqnm (\xn)}$

\begin{align}
 \begin{split}
%
%
  \vnssqr \sqnm(\xn) &= [\sum_{i=1}^n k(s,\xii)\Kinvn_{in}]^2\sqnm(\xn)  \\\\
% 
%
%
  & [k(s, \xn) \bar{c} + < k^{(n-1)}(s) ,  -\bar{c}\Kinvnm k^{(n-1)}(\xn) >]^2\sqnm(\xn)  \\\\
% 
%
%
  &= \frac{[k(s, \xn) - < k^{(n-1)}(s) ,  \Kinvnm k^{(n-1)}(\xn) > ]^2}{\sqnm(\xn)} \\
%
\end{split}
\end{align}

We also rewrite $\sqn(s)$ using matrix block inversion. Now $k = \knm(\xn)$, $K = \Kinvnm $ and $c = k(s,s)$.

\begin{align}
\begin{split}
%
%
\kn(s)\Kinvn \kn(s) &= \knm(s) \bar{K} \knm(s) + 2k(s,\xn)\bar{k}^t\knm(s) + \bar{c}k(s,\xn)^2 \\
%
%
%
% 
&= \knm(s) \bar{K} \knm(s) + 2k(s,\xn)\bar{k}^t\knm(s) + \bar{c}k(s,\xn)^2 \\
%
%
%
%
&= \knm(s) (\Kinvnm + \Kinvnm \knm(\xn)\bar{c} \knm(\xn) \Kinvnm )\knm(s)\\
&\bars  -2k(s,\xn)\bar{c}\knm(\xn)\Kinvnm \knm(s) + \bar{c}k(s,\xn)^2\\
%
%
%
&= \knm(s) \Kinvnm \knm(s) \\
&\bars + \bar{c}[ \knm(s) \Kinvnm \knm(\xn)\knm(\xn) \Kinvnm \knm(s)\\
&\bars  -2k(s,\xn)\knm(\xn)\Kinvnm \knm(s) + k(s,\xn)^2]\\
%
%
%
&= \knm(s) \Kinvnm \knm(s) \\
&\bars + \bar{c}[ (\knm(s) \Kinvnm \knm(\xn))^2\\
&\bars  -2k(s,\xn)\knm(\xn)\Kinvnm \knm(s) + k(s,\xn)^2]\\
%
%
%
&= \knm(s) \Kinvnm \knm(s) \\
&\bars + \frac{1}{\sqnm(\xn)}[ (\knm(s) \Kinvnm \knm(\xn))^2\\
&\bars   -2k(s,\xn)\knm(\xn)\Kinvnm \knm(s) + k(s,\xn)^2]\\
%
%
%
\end{split}
\end{align}

This implies that:

\begin{align}
 \begin{split}
%
%
%
%
  \sqn(s) &= k(s,s) - \kn(s)\Kinvn \kn(s)\\
%
%
%
%
&= \sqnm (s) -\frac{1}{\sqnm(\xn)}[ (\knm(s) \Kinvnm \knm(\xn))^2\\
&\bars   -2k(s,\xn)\knm(\xn)\Kinvnm \knm(s) + k(s,\xn)^2]\\
%
%
%
&= \sqnm (s) -\frac{[ \knm(s) \Kinvnm \knm(\xn) -k(s,\xn)]^2}{\sqnm(\xn)}\\
%
%
%
 \end{split}
\end{align}

Thus:
\begin{align}
 \begin{split}
  \sqn(s) + \frac{ [k(s,\xn) -\knm(s) \Kinvnm \knm(\xn)]^2 }{ \sqnm(\xn)} &= \sqnm(s) \\
 \end{split}
\end{align}

and so we continue the above derivation:

\begin{align}
 \begin{split}
%
%
&=\int \exp( 2\mnm (s) ) (e^{\sqn(s)} -1) \exp( \sqn(s) + \vns^2 \sqnm (\xn) ) ds\\
%
%
%
&=\int \exp( 2\mnm (s) ) (e^{\sqn(s)} -1) \exp( \sqn(s) + \frac{ [k(s,\xn) -\knm(s) \Kinvnm \knm(\xn)]^2 }{ \sqnm(\xn)}  ) ds\\
%
%
%
%
&=\int \exp( 2\mnm (s) ) (e^{\sqn(s)} -1) \exp( \sqnm(s) ) ds\\
%
%
%
%
%
&=\int \exp( 2\mnm (s) +\sqnm(s) ) (e^{\sqn(s)} -1) ds\\
%
%
 \end{split}
\end{align}

Recall that we would like to optimize the above quantity. We use SAA. We take a sample $Z_j \sim \exp( 2\mnm (s) +2\sqnm(s) )$.

\begin{align}
 \begin{split}
  &\argmin_{\xn} \en\int  \var[ \exp( \mn(s) )]ds \\
%
%
%
%
&=\argmin_{\xn}\int \exp( 2\mnm (s) +\sqnm(s) ) (e^{\sqn(s)} -1) ds\\
%
%
%
%
&=\argmin_{\xn}\int \exp( 2\mnm (s) +\sqnm(s) )\exp( \sqnm (s) \\
&\bars -\frac{[ \knm(s) \Kinvnm \knm(\xn) -k(s,\xn)]^2}{\sqnm(\xn)})  ds\\
%
%
%
&=\argmin_{\xn}\int \exp( 2\mnm (s) +2\sqnm(s) )e^{-\frac{[ \knm(s) \Kinvnm \knm(\xn) -k(s,\xn)]^2}{\sqnm(\xn)}}  ds\\
%
%
%
%
&\approx \argmin_{\xn} \frac{1}{N}\sum_{j=1}^{N} \exp(-\frac{[ \knm(Z_j) \Kinvnm \knm(\xn) -k(Z_j,\xn)]^2}{\sqnm(\xn)})\\
%
%
%
%
 \end{split}
\end{align}


\section{Conclusion}
Recall:
\begin{itemize}
 \item $u(\xn,Z_j) = k(Z_j, \xn)  - < \knm (Z_j) ,  \Kinvnm \knm (\xn) >$.
 \item $g(\xn, Z_j) = \frac{u^2}{\sqnm( \xn )} $.
 \item We take $Z_j \sim \exp( 2\mnm (\cdot) +2\sqnm(\cdot) ), j=1,...,N$.
\item $\gxn g(\xn,Z_j) =\frac{ 2u(\xn, Z_j)}{\sqnm(\xn)} \gxn [k(s , \xn) - u(\xn, Z_j)] - \frac{u^2(\xn,Z_j)}{\sqnm(\xn)^2}\gxn \sqnm(\xn)$.
 \end{itemize}


We seek:
$$
\argmin_{\xn} \sum_{j=1}^{N} \exp(-g(\xn, Z_j)).
$$

The gradient is:
$$
-\sum_{j=1}^{N} \exp(-g(\xn,Z_j)) \gxn g(\xn, Z_j).
$$

\end{document}

